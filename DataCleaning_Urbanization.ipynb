{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Cleaning and Urban separation\n",
    "\n",
    "This notebook is first used to clean twitter data and keep only useful information. \n",
    "\n",
    "It then produce, using the cleaned data, a \"Urban mask\" on the tweets. The mask will give the information if the tweet was written in a urban area of switzerland, or in a non-urban area.\n",
    "\n",
    "Using only non-urban tweets, a k-mean clustering algorithm is run in order to detect area of interests in non-urban zone of Switzerland. \n",
    "\n",
    "This script produces a tsv file containing for each non-urban area: The id, date, latitude, longitude, and the cluster it has been assigned too. Another tsv file containing clusters information is produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "# Install GDAL for python to read TIFF urban mask :\n",
    "#conda install -c conda-forge gdal=2.1.2\n",
    "import gdal \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import\n",
    "\n",
    "The data twex.tsv is read line by line. Each row should contain 20 elements. Since there are unclean data, it occurs that some tweets are separated on several line. Since there are around 20Millions of tweets, we can afford do discard the unclean ones.\n",
    "\n",
    "We therefore only keep the rows with exactly 20 elements, and keep only the id, date, latitude, longitude of the tweet.\n",
    "\n",
    "*PS: The code has been commented to avoid running each time, the next cell loads directly the clean file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npath = \\'data/twex.tsv\\'\\nnb_fields = 20\\ncleanedPath = \"data/cleaned.tsv\"\\nwith open(path) as infile:\\n    with open(cleanedPath, \"w\") as outputfile:\\n        i=0\\n        for line in infile:\\n            i = i+1\\n            if i%200000 == 0:\\n                print(\"line:\", i)\\n            line_array = line.split(\"\\t\")\\n            if(len(line_array) == nb_fields):\\n                if line_array[4] != \"\\\\N\":\\n                    outputfile.write(line_array[0])\\n                    outputfile.write(\"\\t\")\\n                    outputfile.write(line_array[2])\\n                    outputfile.write(\"\\t\")\\n                    outputfile.write(line_array[4])\\n                    outputfile.write(\"\\t\")\\n                    outputfile.write(line_array[5])\\n                    outputfile.write(\"\\n\")\\n        outputfile.close()\\n    infile.close()'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "path = 'data/twex.tsv'\n",
    "nb_fields = 20\n",
    "cleanedPath = \"data/cleaned.tsv\"\n",
    "with open(path) as infile:\n",
    "    with open(cleanedPath, \"w\") as outputfile:\n",
    "        i=0\n",
    "        for line in infile:\n",
    "            i = i+1\n",
    "            if i%200000 == 0:\n",
    "                print(\"line:\", i)\n",
    "            line_array = line.split(\"\\t\")\n",
    "            if(len(line_array) == nb_fields):\n",
    "                if line_array[4] != \"\\\\N\":\n",
    "                    outputfile.write(line_array[0])\n",
    "                    outputfile.write(\"\\t\")\n",
    "                    outputfile.write(line_array[2])\n",
    "                    outputfile.write(\"\\t\")\n",
    "                    outputfile.write(line_array[4])\n",
    "                    outputfile.write(\"\\t\")\n",
    "                    outputfile.write(line_array[5])\n",
    "                    outputfile.write(\"\\n\")\n",
    "        outputfile.close()\n",
    "    infile.close()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame.from_csv(\"data/cleaned.tsv\", sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.columns=[\"date\",\"longitude\",\"latitude\"]\n",
    "localisation = data[[\"latitude\",\"longitude\"]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.as_matrix().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Urban Mask\n",
    "\n",
    "### Reading mask file\n",
    "\n",
    "We read the urban mask in TIFF format generated by the Google Earth engine code, based on NDBI of Switzerland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver:  GTiff / GeoTIFF\n",
      "Size is  10521 x 4661 x 1\n",
      "Projection is  GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433],AUTHORITY[\"EPSG\",\"4326\"]]\n",
      "Origin = ( 5.880821007488448 , 47.886043692917276 )\n",
      "Pixel Size = ( 0.00044915764205976077 , -0.00044915764205976077 )\n"
     ]
    }
   ],
   "source": [
    "dataset = gdal.Open( \"data/urban_mask.tif\" )\n",
    "\n",
    "\n",
    "print('Driver: ', dataset.GetDriver().ShortName,'/', \\\n",
    "      dataset.GetDriver().LongName)\n",
    "print( 'Size is ',dataset.RasterXSize,'x',dataset.RasterYSize, \\\n",
    "      'x',dataset.RasterCount)\n",
    "print( 'Projection is ',dataset.GetProjection())\n",
    "geotransform = dataset.GetGeoTransform()\n",
    "if not geotransform is None:\n",
    "    print( 'Origin = (',geotransform[0], ',',geotransform[3],')')\n",
    "    print( 'Pixel Size = (',geotransform[1], ',',geotransform[5],')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the base information about the mask image, and display the two extreme locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topLeft: 5.880821007488448 47.886043692917276\n",
      "bottomRight: 10.606408559599192 45.79251992327673\n"
     ]
    }
   ],
   "source": [
    "srcband = dataset.GetRasterBand(1)\n",
    "#stats = srcband.GetStatistics( True, True )\n",
    "\n",
    "mask = srcband.ReadAsArray(0, 0, srcband.XSize, srcband.YSize)\n",
    "\n",
    "topleftX = geotransform[0]\n",
    "topleftY = geotransform[3]\n",
    "pixelsizeX = geotransform[1]\n",
    "pixelsizeY = geotransform[5]\n",
    "\n",
    "bottomrightX = topleftX+(pixelsizeX*dataset.RasterXSize)\n",
    "bottomrightY = topleftY+(pixelsizeY*dataset.RasterYSize)\n",
    "\n",
    "\n",
    "print(\"topLeft:\", topleftX, topleftY)\n",
    "print(\"bottomRight:\", bottomrightX, bottomrightY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save/Load the mask as numpy array to be reused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save(\"data/urban_mask.npy\", mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask = np.load('data/urban_mask.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the rate of Urban and non Urban area on the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urban area represents: 23.951924514 %\n",
      "Non-urban area represents: 76.048075486 %\n"
     ]
    }
   ],
   "source": [
    "mask_surface = mask.shape[0]*mask.shape[1]\n",
    "urban_rate = mask.sum()/mask_surface\n",
    "print(\"Urban area represents:\", urban_rate*100, \"%\")\n",
    "print(\"Non-urban area represents:\", 100-(urban_rate*100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Filtering tweets\n",
    "\n",
    "In order to filter the tweets by Urban/non-urban Zone, we apply a function to each of them that will use the urban-mask and the tweet location. \n",
    "\n",
    "We add a column \"used\" to the dataframe, when the tweet is in non-urban area used is True, when tweet is in urban area, used is false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Return true if long/lat is located in non-urban area, false otherwise\n",
    "def getValue(long, lat): \n",
    "    xPos = int(1.0 * (long - topleftX) / (bottomrightX - topleftX) * srcband.XSize)\n",
    "    yPos = srcband.YSize-1  - int(1.0 * (lat - bottomrightY) / (topleftY - bottomrightY) * srcband.YSize)\n",
    "    if (xPos >= srcband.XSize or xPos < 0):\n",
    "        return False\n",
    "    if (yPos >= srcband.YSize or yPos < 0):\n",
    "        return False\n",
    "    return mask[yPos, xPos] == 0\n",
    "\n",
    "#Return true if tweet is located in non-urban area, false otherwise\n",
    "def getValueFrame(row): \n",
    "    long = row[\"longitude\"]\n",
    "    lat = row[\"latitude\"]\n",
    "    return getValue(long, lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>used</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9514097914</th>\n",
       "      <td>2010-02-23 05:55:51</td>\n",
       "      <td>7.43926</td>\n",
       "      <td>46.9489</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9514846412</th>\n",
       "      <td>2010-02-23 06:22:40</td>\n",
       "      <td>8.53781</td>\n",
       "      <td>47.3678</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9516574359</th>\n",
       "      <td>2010-02-23 07:34:25</td>\n",
       "      <td>6.13396</td>\n",
       "      <td>46.1951</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9516952605</th>\n",
       "      <td>2010-02-23 07:51:47</td>\n",
       "      <td>8.81749</td>\n",
       "      <td>47.2288</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9517198943</th>\n",
       "      <td>2010-02-23 08:02:57</td>\n",
       "      <td>6.63254</td>\n",
       "      <td>46.5199</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           date  longitude  latitude   used\n",
       "0                                                          \n",
       "9514097914  2010-02-23 05:55:51    7.43926   46.9489  False\n",
       "9514846412  2010-02-23 06:22:40    8.53781   47.3678  False\n",
       "9516574359  2010-02-23 07:34:25    6.13396   46.1951  False\n",
       "9516952605  2010-02-23 07:51:47    8.81749   47.2288  False\n",
       "9517198943  2010-02-23 08:02:57    6.63254   46.5199  False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urbanized = data\n",
    "urbanized[\"used\"] = urbanized.apply(getValueFrame, axis=1)\n",
    "urbanized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute and display the rate and numbers of used tweets (Tweets in non-urban area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate of tweets in non-urban areas: 9.64230761222\n",
      "Number of tweets in non-urban areas: 1.199362 Millions\n"
     ]
    }
   ],
   "source": [
    "used_tweets_rate = (urbanized[urbanized['used'] == True].count()/urbanized.count())[\"used\"]\n",
    "\n",
    "print(\"Rate of tweets in non-urban areas:\", used_tweets_rate*100)\n",
    "print(\"Number of tweets in non-urban areas:\", urbanized[urbanized['used'] == True].count()[\"used\"]/1000000, \"Millions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dataframe of all tweets with \"used\" information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urbanized.to_csv(\"data/cleaned_urbanity_all.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urbanized = pd.DataFrame.from_csv(\"data/cleaned_urbanity_all.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "non_urban = urbanized[urbanized[\"used\"] == True].copy()\n",
    "non_urban.to_csv(\"data/cleaned_non_urban.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>used</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9564875001</th>\n",
       "      <td>2010-02-24 06:26:53</td>\n",
       "      <td>7.44237</td>\n",
       "      <td>46.8958</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9575552318</th>\n",
       "      <td>2010-02-24 13:28:52</td>\n",
       "      <td>8.06674</td>\n",
       "      <td>46.3913</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9575623646</th>\n",
       "      <td>2010-02-24 13:30:51</td>\n",
       "      <td>8.06763</td>\n",
       "      <td>46.3910</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9587557928</th>\n",
       "      <td>2010-02-24 18:45:38</td>\n",
       "      <td>8.77847</td>\n",
       "      <td>47.2034</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9621355274</th>\n",
       "      <td>2010-02-25 11:16:50</td>\n",
       "      <td>7.53729</td>\n",
       "      <td>46.8894</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           date  longitude  latitude  used\n",
       "0                                                         \n",
       "9564875001  2010-02-24 06:26:53    7.44237   46.8958  True\n",
       "9575552318  2010-02-24 13:28:52    8.06674   46.3913  True\n",
       "9575623646  2010-02-24 13:30:51    8.06763   46.3910  True\n",
       "9587557928  2010-02-24 18:45:38    8.77847   47.2034  True\n",
       "9621355274  2010-02-25 11:16:50    7.53729   46.8894  True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_urban.head()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
